{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Native TIMM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your checkpoint file\n",
    "checkpoint_path = \"../model/train_EfficientNetB0_2024-12-04_18-34-06/timm/timm_image/pytorch_model.bin\"\n",
    "image_path = \"e:\\\\Current_Workdir\\\\palm-fruit-classification\\\\data\\\\intermediate\\\\valid\\\\empty_bunch\\\\IMG_20220803_112710_crop_0_jpg.rf.bfef2ca25d24fefe9a8c64c68c5bb66f.jpg\"\n",
    "\n",
    "# Configuration from your JSON\n",
    "config = {\n",
    "    \"architecture\": \"efficientnet_b0\",  # EfficientNet B0 model\n",
    "    \"num_classes\": 6,                   # Number of classes for your task\n",
    "    \"num_features\": 1280,               # Features size for EfficientNet B0\n",
    "    \"pretrained_cfg\": {\n",
    "        \"tag\": \"ra_in1k\",               # Pretraining on ImageNet (RA)\n",
    "        \"custom_load\": False,\n",
    "        \"input_size\": [3, 224, 224],    # Input size\n",
    "        \"fixed_input_size\": False,\n",
    "        \"interpolation\": \"bicubic\",\n",
    "        \"crop_pct\": 0.875,              # Crop percentage for training\n",
    "        \"crop_mode\": \"center\",          # Center cropping\n",
    "        \"mean\": [0.485, 0.456, 0.406],  # ImageNet mean values\n",
    "        \"std\": [0.229, 0.224, 0.225],   # ImageNet std values\n",
    "        \"num_classes\": 1000,            # Default for ImageNet\n",
    "        \"pool_size\": [7, 7],            # Pooling size after convolution\n",
    "        \"first_conv\": \"conv_stem\",      # First convolutional layer\n",
    "        \"classifier\": \"classifier\"      # Final classifier layer\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\comp4\\AppData\\Local\\Temp\\ipykernel_15116\\914206378.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))  # Adjust map_location as needed\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EfficientNet:\n\tMissing key(s) in state_dict: \"classifier.weight\", \"classifier.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Load weights into the model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use strict=True for strict matching\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Example preprocessing pipeline\u001b[39;00m\n\u001b[0;32m     28\u001b[0m input_size \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# (224, 224)\u001b[39;00m\n",
      "File \u001b[1;32me:\\Programs\\miniforge3\\envs\\autogluon_stable_112\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EfficientNet:\n\tMissing key(s) in state_dict: \"classifier.weight\", \"classifier.bias\". "
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Instantiate the model\n",
    "model = timm.create_model(\n",
    "    config[\"architecture\"],  # Model architecture (EfficientNet B0)\n",
    "    pretrained=False,        # Skip loading pretrained weights from timm\n",
    "    num_classes=config[\"num_classes\"],  # Adjust final layer for 6 classes\n",
    "    global_pool=\"avg\",       # Set global pooling (default is \"avg\" for EfficientNet)\n",
    ")\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))  # Adjust map_location as needed\n",
    "if \"state_dict\" in checkpoint:\n",
    "    state_dict = checkpoint[\"state_dict\"]  # For structured checkpoint files\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "# Strip prefixes if necessary (e.g., 'module.' when using DataParallel)\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "# Load weights into the model\n",
    "model.load_state_dict(state_dict, strict=True)  # Use strict=True for strict matching\n",
    "\n",
    "# Example preprocessing pipeline\n",
    "input_size = config[\"pretrained_cfg\"][\"input_size\"][1:]  # (224, 224)\n",
    "mean = config[\"pretrained_cfg\"][\"mean\"]\n",
    "std = config[\"pretrained_cfg\"][\"std\"]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(input_size, interpolation=transforms.InterpolationMode.BICUBIC),  # Resize the image to 224x224\n",
    "    transforms.CenterCrop(int(input_size[0] * config[\"pretrained_cfg\"][\"crop_pct\"])),   # Apply center crop\n",
    "    transforms.ToTensor(),                                                              # Convert image to tensor\n",
    "    transforms.Normalize(mean=mean, std=std),                                            # Normalize using ImageNet mean and std\n",
    "])\n",
    "\n",
    "# Example image (replace with your own image file path)\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Inference\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor)  # Forward pass\n",
    "    predictions = torch.softmax(outputs, dim=1)  # Convert logits to probabilities\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv_stem.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'blocks.0.0.conv_dw.weight', 'blocks.0.0.bn1.weight', 'blocks.0.0.bn1.bias', 'blocks.0.0.bn1.running_mean', 'blocks.0.0.bn1.running_var', 'blocks.0.0.bn1.num_batches_tracked', 'blocks.0.0.se.conv_reduce.weight', 'blocks.0.0.se.conv_reduce.bias', 'blocks.0.0.se.conv_expand.weight', 'blocks.0.0.se.conv_expand.bias', 'blocks.0.0.conv_pw.weight', 'blocks.0.0.bn2.weight', 'blocks.0.0.bn2.bias', 'blocks.0.0.bn2.running_mean', 'blocks.0.0.bn2.running_var', 'blocks.0.0.bn2.num_batches_tracked', 'blocks.1.0.conv_pw.weight', 'blocks.1.0.bn1.weight', 'blocks.1.0.bn1.bias', 'blocks.1.0.bn1.running_mean', 'blocks.1.0.bn1.running_var', 'blocks.1.0.bn1.num_batches_tracked', 'blocks.1.0.conv_dw.weight', 'blocks.1.0.bn2.weight', 'blocks.1.0.bn2.bias', 'blocks.1.0.bn2.running_mean', 'blocks.1.0.bn2.running_var', 'blocks.1.0.bn2.num_batches_tracked', 'blocks.1.0.se.conv_reduce.weight', 'blocks.1.0.se.conv_reduce.bias', 'blocks.1.0.se.conv_expand.weight', 'blocks.1.0.se.conv_expand.bias', 'blocks.1.0.conv_pwl.weight', 'blocks.1.0.bn3.weight', 'blocks.1.0.bn3.bias', 'blocks.1.0.bn3.running_mean', 'blocks.1.0.bn3.running_var', 'blocks.1.0.bn3.num_batches_tracked', 'blocks.1.1.conv_pw.weight', 'blocks.1.1.bn1.weight', 'blocks.1.1.bn1.bias', 'blocks.1.1.bn1.running_mean', 'blocks.1.1.bn1.running_var', 'blocks.1.1.bn1.num_batches_tracked', 'blocks.1.1.conv_dw.weight', 'blocks.1.1.bn2.weight', 'blocks.1.1.bn2.bias', 'blocks.1.1.bn2.running_mean', 'blocks.1.1.bn2.running_var', 'blocks.1.1.bn2.num_batches_tracked', 'blocks.1.1.se.conv_reduce.weight', 'blocks.1.1.se.conv_reduce.bias', 'blocks.1.1.se.conv_expand.weight', 'blocks.1.1.se.conv_expand.bias', 'blocks.1.1.conv_pwl.weight', 'blocks.1.1.bn3.weight', 'blocks.1.1.bn3.bias', 'blocks.1.1.bn3.running_mean', 'blocks.1.1.bn3.running_var', 'blocks.1.1.bn3.num_batches_tracked', 'blocks.2.0.conv_pw.weight', 'blocks.2.0.bn1.weight', 'blocks.2.0.bn1.bias', 'blocks.2.0.bn1.running_mean', 'blocks.2.0.bn1.running_var', 'blocks.2.0.bn1.num_batches_tracked', 'blocks.2.0.conv_dw.weight', 'blocks.2.0.bn2.weight', 'blocks.2.0.bn2.bias', 'blocks.2.0.bn2.running_mean', 'blocks.2.0.bn2.running_var', 'blocks.2.0.bn2.num_batches_tracked', 'blocks.2.0.se.conv_reduce.weight', 'blocks.2.0.se.conv_reduce.bias', 'blocks.2.0.se.conv_expand.weight', 'blocks.2.0.se.conv_expand.bias', 'blocks.2.0.conv_pwl.weight', 'blocks.2.0.bn3.weight', 'blocks.2.0.bn3.bias', 'blocks.2.0.bn3.running_mean', 'blocks.2.0.bn3.running_var', 'blocks.2.0.bn3.num_batches_tracked', 'blocks.2.1.conv_pw.weight', 'blocks.2.1.bn1.weight', 'blocks.2.1.bn1.bias', 'blocks.2.1.bn1.running_mean', 'blocks.2.1.bn1.running_var', 'blocks.2.1.bn1.num_batches_tracked', 'blocks.2.1.conv_dw.weight', 'blocks.2.1.bn2.weight', 'blocks.2.1.bn2.bias', 'blocks.2.1.bn2.running_mean', 'blocks.2.1.bn2.running_var', 'blocks.2.1.bn2.num_batches_tracked', 'blocks.2.1.se.conv_reduce.weight', 'blocks.2.1.se.conv_reduce.bias', 'blocks.2.1.se.conv_expand.weight', 'blocks.2.1.se.conv_expand.bias', 'blocks.2.1.conv_pwl.weight', 'blocks.2.1.bn3.weight', 'blocks.2.1.bn3.bias', 'blocks.2.1.bn3.running_mean', 'blocks.2.1.bn3.running_var', 'blocks.2.1.bn3.num_batches_tracked', 'blocks.3.0.conv_pw.weight', 'blocks.3.0.bn1.weight', 'blocks.3.0.bn1.bias', 'blocks.3.0.bn1.running_mean', 'blocks.3.0.bn1.running_var', 'blocks.3.0.bn1.num_batches_tracked', 'blocks.3.0.conv_dw.weight', 'blocks.3.0.bn2.weight', 'blocks.3.0.bn2.bias', 'blocks.3.0.bn2.running_mean', 'blocks.3.0.bn2.running_var', 'blocks.3.0.bn2.num_batches_tracked', 'blocks.3.0.se.conv_reduce.weight', 'blocks.3.0.se.conv_reduce.bias', 'blocks.3.0.se.conv_expand.weight', 'blocks.3.0.se.conv_expand.bias', 'blocks.3.0.conv_pwl.weight', 'blocks.3.0.bn3.weight', 'blocks.3.0.bn3.bias', 'blocks.3.0.bn3.running_mean', 'blocks.3.0.bn3.running_var', 'blocks.3.0.bn3.num_batches_tracked', 'blocks.3.1.conv_pw.weight', 'blocks.3.1.bn1.weight', 'blocks.3.1.bn1.bias', 'blocks.3.1.bn1.running_mean', 'blocks.3.1.bn1.running_var', 'blocks.3.1.bn1.num_batches_tracked', 'blocks.3.1.conv_dw.weight', 'blocks.3.1.bn2.weight', 'blocks.3.1.bn2.bias', 'blocks.3.1.bn2.running_mean', 'blocks.3.1.bn2.running_var', 'blocks.3.1.bn2.num_batches_tracked', 'blocks.3.1.se.conv_reduce.weight', 'blocks.3.1.se.conv_reduce.bias', 'blocks.3.1.se.conv_expand.weight', 'blocks.3.1.se.conv_expand.bias', 'blocks.3.1.conv_pwl.weight', 'blocks.3.1.bn3.weight', 'blocks.3.1.bn3.bias', 'blocks.3.1.bn3.running_mean', 'blocks.3.1.bn3.running_var', 'blocks.3.1.bn3.num_batches_tracked', 'blocks.3.2.conv_pw.weight', 'blocks.3.2.bn1.weight', 'blocks.3.2.bn1.bias', 'blocks.3.2.bn1.running_mean', 'blocks.3.2.bn1.running_var', 'blocks.3.2.bn1.num_batches_tracked', 'blocks.3.2.conv_dw.weight', 'blocks.3.2.bn2.weight', 'blocks.3.2.bn2.bias', 'blocks.3.2.bn2.running_mean', 'blocks.3.2.bn2.running_var', 'blocks.3.2.bn2.num_batches_tracked', 'blocks.3.2.se.conv_reduce.weight', 'blocks.3.2.se.conv_reduce.bias', 'blocks.3.2.se.conv_expand.weight', 'blocks.3.2.se.conv_expand.bias', 'blocks.3.2.conv_pwl.weight', 'blocks.3.2.bn3.weight', 'blocks.3.2.bn3.bias', 'blocks.3.2.bn3.running_mean', 'blocks.3.2.bn3.running_var', 'blocks.3.2.bn3.num_batches_tracked', 'blocks.4.0.conv_pw.weight', 'blocks.4.0.bn1.weight', 'blocks.4.0.bn1.bias', 'blocks.4.0.bn1.running_mean', 'blocks.4.0.bn1.running_var', 'blocks.4.0.bn1.num_batches_tracked', 'blocks.4.0.conv_dw.weight', 'blocks.4.0.bn2.weight', 'blocks.4.0.bn2.bias', 'blocks.4.0.bn2.running_mean', 'blocks.4.0.bn2.running_var', 'blocks.4.0.bn2.num_batches_tracked', 'blocks.4.0.se.conv_reduce.weight', 'blocks.4.0.se.conv_reduce.bias', 'blocks.4.0.se.conv_expand.weight', 'blocks.4.0.se.conv_expand.bias', 'blocks.4.0.conv_pwl.weight', 'blocks.4.0.bn3.weight', 'blocks.4.0.bn3.bias', 'blocks.4.0.bn3.running_mean', 'blocks.4.0.bn3.running_var', 'blocks.4.0.bn3.num_batches_tracked', 'blocks.4.1.conv_pw.weight', 'blocks.4.1.bn1.weight', 'blocks.4.1.bn1.bias', 'blocks.4.1.bn1.running_mean', 'blocks.4.1.bn1.running_var', 'blocks.4.1.bn1.num_batches_tracked', 'blocks.4.1.conv_dw.weight', 'blocks.4.1.bn2.weight', 'blocks.4.1.bn2.bias', 'blocks.4.1.bn2.running_mean', 'blocks.4.1.bn2.running_var', 'blocks.4.1.bn2.num_batches_tracked', 'blocks.4.1.se.conv_reduce.weight', 'blocks.4.1.se.conv_reduce.bias', 'blocks.4.1.se.conv_expand.weight', 'blocks.4.1.se.conv_expand.bias', 'blocks.4.1.conv_pwl.weight', 'blocks.4.1.bn3.weight', 'blocks.4.1.bn3.bias', 'blocks.4.1.bn3.running_mean', 'blocks.4.1.bn3.running_var', 'blocks.4.1.bn3.num_batches_tracked', 'blocks.4.2.conv_pw.weight', 'blocks.4.2.bn1.weight', 'blocks.4.2.bn1.bias', 'blocks.4.2.bn1.running_mean', 'blocks.4.2.bn1.running_var', 'blocks.4.2.bn1.num_batches_tracked', 'blocks.4.2.conv_dw.weight', 'blocks.4.2.bn2.weight', 'blocks.4.2.bn2.bias', 'blocks.4.2.bn2.running_mean', 'blocks.4.2.bn2.running_var', 'blocks.4.2.bn2.num_batches_tracked', 'blocks.4.2.se.conv_reduce.weight', 'blocks.4.2.se.conv_reduce.bias', 'blocks.4.2.se.conv_expand.weight', 'blocks.4.2.se.conv_expand.bias', 'blocks.4.2.conv_pwl.weight', 'blocks.4.2.bn3.weight', 'blocks.4.2.bn3.bias', 'blocks.4.2.bn3.running_mean', 'blocks.4.2.bn3.running_var', 'blocks.4.2.bn3.num_batches_tracked', 'blocks.5.0.conv_pw.weight', 'blocks.5.0.bn1.weight', 'blocks.5.0.bn1.bias', 'blocks.5.0.bn1.running_mean', 'blocks.5.0.bn1.running_var', 'blocks.5.0.bn1.num_batches_tracked', 'blocks.5.0.conv_dw.weight', 'blocks.5.0.bn2.weight', 'blocks.5.0.bn2.bias', 'blocks.5.0.bn2.running_mean', 'blocks.5.0.bn2.running_var', 'blocks.5.0.bn2.num_batches_tracked', 'blocks.5.0.se.conv_reduce.weight', 'blocks.5.0.se.conv_reduce.bias', 'blocks.5.0.se.conv_expand.weight', 'blocks.5.0.se.conv_expand.bias', 'blocks.5.0.conv_pwl.weight', 'blocks.5.0.bn3.weight', 'blocks.5.0.bn3.bias', 'blocks.5.0.bn3.running_mean', 'blocks.5.0.bn3.running_var', 'blocks.5.0.bn3.num_batches_tracked', 'blocks.5.1.conv_pw.weight', 'blocks.5.1.bn1.weight', 'blocks.5.1.bn1.bias', 'blocks.5.1.bn1.running_mean', 'blocks.5.1.bn1.running_var', 'blocks.5.1.bn1.num_batches_tracked', 'blocks.5.1.conv_dw.weight', 'blocks.5.1.bn2.weight', 'blocks.5.1.bn2.bias', 'blocks.5.1.bn2.running_mean', 'blocks.5.1.bn2.running_var', 'blocks.5.1.bn2.num_batches_tracked', 'blocks.5.1.se.conv_reduce.weight', 'blocks.5.1.se.conv_reduce.bias', 'blocks.5.1.se.conv_expand.weight', 'blocks.5.1.se.conv_expand.bias', 'blocks.5.1.conv_pwl.weight', 'blocks.5.1.bn3.weight', 'blocks.5.1.bn3.bias', 'blocks.5.1.bn3.running_mean', 'blocks.5.1.bn3.running_var', 'blocks.5.1.bn3.num_batches_tracked', 'blocks.5.2.conv_pw.weight', 'blocks.5.2.bn1.weight', 'blocks.5.2.bn1.bias', 'blocks.5.2.bn1.running_mean', 'blocks.5.2.bn1.running_var', 'blocks.5.2.bn1.num_batches_tracked', 'blocks.5.2.conv_dw.weight', 'blocks.5.2.bn2.weight', 'blocks.5.2.bn2.bias', 'blocks.5.2.bn2.running_mean', 'blocks.5.2.bn2.running_var', 'blocks.5.2.bn2.num_batches_tracked', 'blocks.5.2.se.conv_reduce.weight', 'blocks.5.2.se.conv_reduce.bias', 'blocks.5.2.se.conv_expand.weight', 'blocks.5.2.se.conv_expand.bias', 'blocks.5.2.conv_pwl.weight', 'blocks.5.2.bn3.weight', 'blocks.5.2.bn3.bias', 'blocks.5.2.bn3.running_mean', 'blocks.5.2.bn3.running_var', 'blocks.5.2.bn3.num_batches_tracked', 'blocks.5.3.conv_pw.weight', 'blocks.5.3.bn1.weight', 'blocks.5.3.bn1.bias', 'blocks.5.3.bn1.running_mean', 'blocks.5.3.bn1.running_var', 'blocks.5.3.bn1.num_batches_tracked', 'blocks.5.3.conv_dw.weight', 'blocks.5.3.bn2.weight', 'blocks.5.3.bn2.bias', 'blocks.5.3.bn2.running_mean', 'blocks.5.3.bn2.running_var', 'blocks.5.3.bn2.num_batches_tracked', 'blocks.5.3.se.conv_reduce.weight', 'blocks.5.3.se.conv_reduce.bias', 'blocks.5.3.se.conv_expand.weight', 'blocks.5.3.se.conv_expand.bias', 'blocks.5.3.conv_pwl.weight', 'blocks.5.3.bn3.weight', 'blocks.5.3.bn3.bias', 'blocks.5.3.bn3.running_mean', 'blocks.5.3.bn3.running_var', 'blocks.5.3.bn3.num_batches_tracked', 'blocks.6.0.conv_pw.weight', 'blocks.6.0.bn1.weight', 'blocks.6.0.bn1.bias', 'blocks.6.0.bn1.running_mean', 'blocks.6.0.bn1.running_var', 'blocks.6.0.bn1.num_batches_tracked', 'blocks.6.0.conv_dw.weight', 'blocks.6.0.bn2.weight', 'blocks.6.0.bn2.bias', 'blocks.6.0.bn2.running_mean', 'blocks.6.0.bn2.running_var', 'blocks.6.0.bn2.num_batches_tracked', 'blocks.6.0.se.conv_reduce.weight', 'blocks.6.0.se.conv_reduce.bias', 'blocks.6.0.se.conv_expand.weight', 'blocks.6.0.se.conv_expand.bias', 'blocks.6.0.conv_pwl.weight', 'blocks.6.0.bn3.weight', 'blocks.6.0.bn3.bias', 'blocks.6.0.bn3.running_mean', 'blocks.6.0.bn3.running_var', 'blocks.6.0.bn3.num_batches_tracked', 'conv_head.weight', 'bn2.weight', 'bn2.bias', 'bn2.running_mean', 'bn2.running_var', 'bn2.num_batches_tracked', 'classifier.weight', 'classifier.bias'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compared to autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load pretrained checkpoint: e:\\Current_Workdir\\palm-fruit-classification\\autogluon_ver\\model\\train_EfficientNetB2_100_trials_2024-12-06_11-02-58\\model.ckpt\n"
     ]
    }
   ],
   "source": [
    "from autogluon.multimodal import MultiModalPredictor\n",
    "\n",
    "predictor = MultiModalPredictor.load(\"e:\\Current_Workdir\\palm-fruit-classification\\\\autogluon_ver\\model\\\\train_EfficientNetB2_100_trials_2024-12-06_11-02-58\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.5468,  0.3652, -1.3130,  ...,  0.0398,  0.4851,  0.0912],\n",
       "          [ 1.3070,  0.9303, -1.0562,  ...,  0.1426,  0.2111, -0.2856],\n",
       "          [ 1.2557,  1.1187, -1.1075,  ...,  0.1597,  0.0227,  0.0056],\n",
       "          ...,\n",
       "          [ 0.3823,  0.5364,  0.2624,  ...,  1.7180,  1.9578,  2.2489],\n",
       "          [ 1.8379,  1.8722,  2.0092,  ...,  1.9920,  2.2489,  2.0777],\n",
       "          [ 2.2489,  2.2489,  2.0263,  ...,  1.4440,  1.5810,  2.2318]],\n",
       "\n",
       "         [[ 1.4307,  0.2227, -1.4405,  ...,  0.2227,  0.7304,  0.3277],\n",
       "          [ 1.2206,  0.8354, -1.1779,  ...,  0.3102,  0.3978, -0.1099],\n",
       "          [ 1.2381,  1.0980, -1.2129,  ...,  0.3277,  0.1702,  0.1527],\n",
       "          ...,\n",
       "          [-0.1450, -0.0049, -0.3725,  ...,  0.9055,  0.9930,  1.5182],\n",
       "          [ 0.9930,  1.0455,  1.3431,  ...,  1.2381,  1.3431,  0.9405],\n",
       "          [ 1.4832,  1.5357,  1.3782,  ...,  0.6429,  0.4503,  1.0105]],\n",
       "\n",
       "         [[ 1.4200,  0.2173, -1.3687,  ...,  0.3219,  0.7751,  0.3742],\n",
       "          [ 1.2457,  0.8622, -1.1073,  ...,  0.4439,  0.4614, -0.0441],\n",
       "          [ 1.2980,  1.1585, -1.1944,  ...,  0.4962,  0.2871,  0.2696],\n",
       "          ...,\n",
       "          [-0.6018, -0.4973, -0.9330,  ...,  0.8971,  0.8274,  1.2108],\n",
       "          [-0.0615,  0.0256,  0.3045,  ...,  1.3502,  1.4722,  1.0539],\n",
       "          [ 0.0779,  0.1651,  0.0256,  ...,  0.8099,  0.6879,  1.2457]]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.4552076e-04 9.9829370e-01 1.2130731e-03 6.1208465e-07 1.4657085e-04\n",
      "  6.1448026e-07]]\n"
     ]
    }
   ],
   "source": [
    "proba = predictor.predict_proba({'image': [image_path]}, realtime=True)\n",
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['overripe'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predictor.predict({'image': [image_path]}, realtime=True)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([[0.1547, 0.1680, 0.1689, 0.1790, 0.1568, 0.1726]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\comp4\\AppData\\Local\\Temp\\ipykernel_35776\\2161494468.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_filepath)\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "with open(\"../model/train_EfficientNetB2_100_trials_2024-12-06_11-02-58/timm/timm_image/config.json\", 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "checkpoint_filepath = \"../model/train_EfficientNetB2_100_trials_2024-12-06_11-02-58/timm/timm_image/pytorch_model.bin\"\n",
    "\n",
    "# model = timm.create_model(model_name=config[\"architecture\"],\n",
    "#                           pretrained=True,\n",
    "#                           num_classes=config[\"num_classes\"],\n",
    "#                           pretrained_cfg=config[\"pretrained_cfg\"],\n",
    "#                           pretrained_cfg_overlay=dict(file=checkpoint_filepath))\n",
    "\n",
    "\n",
    "\n",
    "# Check the config.json, there are 2 num_classes\n",
    "\n",
    "# model = timm.create_model(\n",
    "#     model_name=\"timm/efficientnet_b2.ra_in1k\",\n",
    "#     num_classes=6,  # Ensure this matches the number of output classes in your model\n",
    "#     checkpoint_path=None\n",
    "# )\n",
    "\n",
    "# Modify the classifier layer if necessary\n",
    "model.classifier = torch.nn.Linear(in_features=model.classifier.in_features, out_features=6)\n",
    "\n",
    "# Load the checkpoint after modifying the classifier\n",
    "checkpoint = torch.load(checkpoint_filepath)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "\n",
    "# Example preprocessing pipeline\n",
    "input_size = config[\"pretrained_cfg\"][\"input_size\"][1:]  # (224, 224)\n",
    "mean = config[\"pretrained_cfg\"][\"mean\"]\n",
    "std = config[\"pretrained_cfg\"][\"std\"]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(input_size, interpolation=transforms.InterpolationMode.BICUBIC),  # Resize the image to 224x224\n",
    "    transforms.CenterCrop(int(input_size[0] * config[\"pretrained_cfg\"][\"crop_pct\"])),   # Apply center crop\n",
    "    transforms.ToTensor(),                                                              # Convert image to tensor\n",
    "    transforms.Normalize(mean=mean, std=std),                                            # Normalize using ImageNet mean and std\n",
    "])\n",
    "\n",
    "# Example image (replace with your own image file path)\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Inference\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor)  # Forward pass\n",
    "    predictions = torch.softmax(outputs, dim=1)  # Convert logits to probabilities\n",
    "\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'efficientnet_b2', 'num_classes': 6, 'num_features': 1408, 'pretrained_cfg': {'tag': 'ra_in1k', 'custom_load': False, 'input_size': [3, 256, 256], 'test_input_size': [3, 288, 288], 'fixed_input_size': False, 'interpolation': 'bicubic', 'crop_pct': 1.0, 'crop_mode': 'center', 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225], 'num_classes': 1000, 'pool_size': [8, 8], 'first_conv': 'conv_stem', 'classifier': 'classifier'}}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogluon_stable_112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
